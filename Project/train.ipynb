{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "loqLueA6ZjsN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.cuda\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import requests, tarfile\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaGoxjrxZjsS",
        "outputId": "8da63076-842f-4c8f-89e4-3c25e8690a3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "iwUbqNHYZjsT"
      },
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "GENDER = ['M', 'F']\n",
        "ETHNICITY = ['White', 'Black', 'Asian', 'Indian', 'Others']\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def arrayAge(data):\n",
        "    res = []\n",
        "    for (age, _, __) in data:\n",
        "        res.append(age)\n",
        "    return np.array(res)\n",
        "\n",
        "def genderToStr(num):\n",
        "    return GENDER[num]\n",
        "\n",
        "def arrayGenderToStr(data):\n",
        "    res = []\n",
        "    for (_, gender, __) in data:\n",
        "        res.append(genderToStr(gender))\n",
        "    return np.array(res)\n",
        "\n",
        "def ethnicityToStr(num):\n",
        "    return ETHNICITY[num]\n",
        "\n",
        "def arrayEthnicityToStr(data):\n",
        "    res = []\n",
        "    for (_, __, ethnicity) in data:\n",
        "        res.append(ethnicityToStr(ethnicity))\n",
        "    return np.array(res)\n",
        "\n",
        "def histPlot(labels, title, yLabel, xLabel, bins):\n",
        "    plt.title(title, size=16)\n",
        "    sns.histplot(x = labels, bins = bins)\n",
        "    plt.ylabel(yLabel, size=12)\n",
        "    plt.xlabel(xLabel, size=12)\n",
        "    sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "    plt.show()\n",
        "\n",
        "def countPlot(labels, title, yLabel, xLabel):\n",
        "    plt.title(title, size=16)\n",
        "    ax = sns.countplot(x = labels)\n",
        "    plt.ylabel(yLabel, size=12)\n",
        "    plt.xlabel(xLabel, size=12)\n",
        "    sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "\n",
        "    total = len(labels)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        percentage = f'{100 * height / total:.1f}%'\n",
        "        ax.text(p.get_x() + p.get_width() / 2,\n",
        "                height + 5,\n",
        "                percentage,\n",
        "                ha='center')\n",
        "        \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FddWCNRzZjsU",
        "outputId": "5259e8a9-458c-41fa-a13f-8ba0594fd305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UTKFace already downloaded.\n"
          ]
        }
      ],
      "source": [
        "dataPath = 'UTKFace'\n",
        "if (dataPath not in os.listdir()):\n",
        "    print(\"Downloading UTKFace...\")\n",
        "    url = \"https://drive.google.com/uc?export=download&id=0BxYys69jI14kYVM3aVhKS1VhRUk&confirm=t&uuid=f981ca1d-ba0f-40c9-a4a0-8eaa887f3b6d&at=ANzk5s7e36SgjT0FlqBbRiijefRg:1681897584880\"\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    file = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
        "    file.extractall(path=\".\")\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(\"UTKFace already downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "SKXElAb3ZjsU"
      },
      "outputs": [],
      "source": [
        "#data = []\n",
        "#labels = []\n",
        "\n",
        "#for imagePath in os.listdir(dataPath):\n",
        "#    try:\n",
        "#        imageTensor = torchvision.io.read_image(f'{dataPath}/{imagePath}').float().half()\n",
        "#        fileName = imagePath.split('_')\n",
        "#        labels.append((int(fileName[0]), int(fileName[1]), int(fileName[2])))\n",
        "#        data.append(imageTensor)\n",
        "#    except:\n",
        "#        pass\n",
        "#data = torch.stack(data).to(device)\n",
        "#labels = torch.Tensor(labels).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "p808DasNdMtZ"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataPath, transform=None):\n",
        "        self.dataPath = dataPath\n",
        "        self.transform = transform\n",
        "        self.imagePaths = [f for f in os.listdir(self.dataPath) if f.endswith('.jpg')]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        imagePath = self.imagePaths[index]\n",
        "        try:\n",
        "            imageTensor = torchvision.io.read_image(f'{self.dataPath}/{imagePath}').float()\n",
        "            fileName = imagePath.split('_')\n",
        "            label = torch.Tensor([int(fileName[0]), int(fileName[1]), int(fileName[2])])\n",
        "            if self.transform:\n",
        "                imageTensor = self.transform(imageTensor)\n",
        "            return imageTensor, label\n",
        "        except:\n",
        "            return self.__getitem__((index + 1) % len(self.imagePaths))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.imagePaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFTvbBTeZjsX",
        "outputId": "e0669894-83b4-4320-b1a7-c177273e9498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23708 2370 21338\n"
          ]
        }
      ],
      "source": [
        "# Load and normalizde the data\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize(224) #,\n",
        "     # transforms.ToTensor(),\n",
        "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "#dataset = torch.utils.data.TensorDataset(data, labels)\n",
        "\n",
        "dataset = MyDataset(dataPath, transform=transform)\n",
        "\n",
        "batchSize = 100\n",
        "testSplit = 0.1 # use 10% of dataset as test\n",
        "validSplit = 0.2 / (1-testSplit) # use 20% of dataset as validation\n",
        "\n",
        "testSize = int(np.floor(len(dataset)*testSplit))\n",
        "trainValidSize = int(np.ceil(len(dataset)*(1-testSplit)))\n",
        "validSize = int(np.ceil(trainValidSize*validSplit))\n",
        "trainSize = int(np.floor(trainValidSize*(1-validSplit)))\n",
        "print(len(dataset), testSize, trainValidSize)\n",
        "\n",
        "trainValidSet, testSet = torch.utils.data.random_split(dataset, [trainValidSize, testSize])\n",
        "trainSet, validSet = torch.utils.data.random_split(trainValidSet, [trainSize, validSize])\n",
        "\n",
        "trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize, shuffle=True)\n",
        "validLoader = torch.utils.data.DataLoader(validSet, batch_size=batchSize, shuffle=True)\n",
        "testLoader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "rOCaZIQkZjsY"
      },
      "outputs": [],
      "source": [
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetModel,self).__init__()\n",
        "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad=False\n",
        "        self.resnet.fc = nn.Linear(512, 512)\n",
        "        self.ageFc = nn.Linear(512,1)\n",
        "        self.genderFc = nn.Linear(512,2)\n",
        "        self.ethnicityFc = nn.Linear(512,5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        resOut = F.relu(self.resnet.forward(x))\n",
        "        ageOut = self.ageFc.forward(resOut)\n",
        "        #genderOut = self.ageFc.forward(resOut)\n",
        "        ethnicityOut = self.ageFc.forward(resOut)\n",
        "        genderOut = F.sigmoid(self.ageFc.forward(resOut))\n",
        "        #ethnicityOut = F.softmax(self.ageFc.forward(resOut))\n",
        "        return ageOut, genderOut, ethnicityOut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "EyEWr79LZjsY"
      },
      "outputs": [],
      "source": [
        "# ResNetModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "PPTkQJzyZjsY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lossAge(predictAge, targetAge):\n",
        "    loss = F.mse_loss(predictAge, targetAge)\n",
        "    return loss\n",
        "\n",
        "def lossGender(predictGender, targetGender):\n",
        "    loss = F.binary_cross_entropy(predictGender, targetGender)\n",
        "    return loss\n",
        "\n",
        "def lossEthnicity(predictEthnicity, targetEthnicity):\n",
        "    loss = F.cross_entropy(predictEthnicity, targetEthnicity)\n",
        "    return loss\n",
        "\n",
        "def lossFunction(predictAge, predictGender, predictEthnicity, targetAge, targetGender, targetEthnicity):\n",
        "    alpha = 1/3 # weight for age prediction\n",
        "    beta = 1/3 # weight for gender prediction\n",
        "    gamma = 1/3 # weight for ethncity prediction\n",
        "    ageLoss = lossAge(predictAge, targetAge)\n",
        "    genderLoss = lossGender(predictGender, targetGender)\n",
        "    ethnicityLoss = lossEthnicity(predictEthnicity, targetEthnicity)\n",
        "    totalLoss = alpha * ageLoss + beta * genderLoss + gamma * ethnicityLoss\n",
        "    return totalLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "VyGFkzdvZjsZ"
      },
      "outputs": [],
      "source": [
        "def trainNetwork(model, optimizer, lossFunction, trainLoader, validLoader, epochs, device):\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        \n",
        "        ### TRAINING ###\n",
        "        trainLoss = 0\n",
        "        correctTrain = 0\n",
        "        totalTrain = 0\n",
        "        for batch_nr, (images, labels) in enumerate(trainLoader):\n",
        "            # Move data to GPU (if exists)\n",
        "            images, labels = images.to(device), labels.to(device)  \n",
        "\n",
        "            # Predict\n",
        "            agePredictions, genderPredictions, ethnicityPredictions = model(images)\n",
        "\n",
        "            # Get loss and backpropogate\n",
        "            loss = lossFunction(agePredictions, genderPredictions, ethnicityPredictions, \n",
        "                                labels[:, 0].view(-1, 1), labels[:, 1].view(-1, 1), labels[:, 2].view(-1, 1))\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimize parameters (weights and biases) and remove gradients after\n",
        "            optimizer.step() \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Save loss for whole epoch\n",
        "            trainLoss += loss.item()\n",
        "            \n",
        "            # Calculate training accuracy\n",
        "            # _, predictions = torch.max(predictions, 1) \n",
        "            # correctTrain += (predictions == labels).sum().item() \n",
        "            # totalTrain += len(images)\n",
        "\n",
        "            #print(f'Epoch [{epoch+1}/{epochs}] Batch [{batch_nr}/{len(trainLoader)}]')\n",
        "        \n",
        "\n",
        "        trainLoss /= len(trainLoader)\n",
        "        trainAccuracy = 100 * correctTrain / totalTrain\n",
        "\n",
        "        ### VALIDATION ###\n",
        "        validLoss = 0\n",
        "        correctValid = 0\n",
        "        totalValid = 0\n",
        "        for batch_nr, (images, labels) in enumerate(validLoader):\n",
        "            # Move data to GPU (if exists)\n",
        "            images, labels = images.to(device), labels.to(device) \n",
        "\n",
        "            # Predict            \n",
        "            agePredictions, genderPredictions, ethnicityPredictions = model(images)\n",
        "\n",
        "            # Get loss\n",
        "            loss = lossFunction(agePredictions, genderPredictions, ethnicityPredictions, \n",
        "                                labels[:, 0].view(-1, 1), labels[:, 1].view(-1, 1), labels[:, 2].view(-1, 1))\n",
        "\n",
        "            # Save loss for whole epoch\n",
        "            validLoss += loss.item()\n",
        "\n",
        "            # Calculate vaildation accuracy\n",
        "            #_, predictions = torch.max(predictions, 1) \n",
        "            #correctValid += (predictions == labels).sum().item() \n",
        "            #totalValid += len(images)\n",
        "\n",
        "            #print(f'Epoch [{epoch+1}/{epochs}] Batch [{batch_nr}/{len(validLoader)}]')\n",
        "\n",
        "        validLoss /= len(validLoader)\n",
        "        validAccuracy = 100 * correctValid / totalValid\n",
        "\n",
        "        # Print reuslt of epoch\n",
        "        print(f'Epoch [{epoch}/{epochs}] \\t Training Loss: {round(trainLoss, 4)} \\t Validation Loss: {round(validLoss, 4)} \\t Traning Acc: {round(trainAccuracy, 2)}% \\t Validation Acc: {round(validAccuracy, 2)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "shpRsm7hi3bg",
        "outputId": "b4ccab68-1dc0-4c75-d438-6eafb095c35b"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[78], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m resnetModel \u001b[39m=\u001b[39m ResNetModel()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(resnetModel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearningRate)\n\u001b[1;32m----> 6\u001b[0m trainNetwork(resnetModel, optimizer, lossFunction, trainLoader, validLoader, epochs, device)\n",
            "Cell \u001b[1;32mIn[77], line 14\u001b[0m, in \u001b[0;36mtrainNetwork\u001b[1;34m(model, optimizer, lossFunction, trainLoader, validLoader, epochs, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)  \n\u001b[0;32m     13\u001b[0m \u001b[39m# Predict\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m agePredictions, genderPredictions, ethnicityPredictions \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     16\u001b[0m \u001b[39m# Get loss and backpropogate\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[39m=\u001b[39m lossFunction(agePredictions, genderPredictions, ethnicityPredictions, \n\u001b[0;32m     18\u001b[0m                     labels[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), labels[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), labels[:, \u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\isakl\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[74], line 13\u001b[0m, in \u001b[0;36mResNetModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     resOut \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresnet\u001b[39m.\u001b[39;49mforward(x))\n\u001b[0;32m     14\u001b[0m     ageOut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mageFc\u001b[39m.\u001b[39mforward(resOut)\n\u001b[0;32m     15\u001b[0m     \u001b[39m#genderOut = self.ageFc.forward(resOut)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\isakl\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\torchvision\\models\\resnet.py:249\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
            "File \u001b[1;32mc:\\Users\\isakl\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\torchvision\\models\\resnet.py:238\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    235\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m    237\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[1;32m--> 238\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x)\n\u001b[0;32m    239\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[0;32m    240\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n",
            "File \u001b[1;32mc:\\Users\\isakl\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\isakl\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\isakl\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\isakl\\.pyenv\\pyenv-win\\versions\\3.8.10\\lib\\site-packages\\torchvision\\models\\resnet.py:80\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(x)\n\u001b[1;32m---> 80\u001b[0m out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m identity\n\u001b[0;32m     81\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m     83\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 1\n",
        "learningRate = 1e-3\n",
        "resnetModel = ResNetModel().to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(resnetModel.parameters(), lr=learningRate)\n",
        "trainNetwork(resnetModel, optimizer, lossFunction, trainLoader, validLoader, epochs, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtfZSqAgzatB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvcc' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0e46589cfc653cddf5fb35f750697a95da7a749fba015bbad1355989ab2e8d35"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
