{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "loqLueA6ZjsN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.cuda\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sklearn as sk\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import requests, tarfile\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PaGoxjrxZjsS",
        "outputId": "8da63076-842f-4c8f-89e4-3c25e8690a3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iwUbqNHYZjsT"
      },
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "GENDER = ['M', 'F']\n",
        "ETHNICITY = ['White', 'Black', 'Asian', 'Indian', 'Others']\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def arrayAge(data):\n",
        "    res = []\n",
        "    for (age, _, __) in data:\n",
        "        res.append(age)\n",
        "    return np.array(res)\n",
        "\n",
        "def genderToStr(num):\n",
        "    return GENDER[num]\n",
        "\n",
        "def arrayGenderToStr(data):\n",
        "    res = []\n",
        "    for (_, gender, __) in data:\n",
        "        res.append(genderToStr(gender))\n",
        "    return np.array(res)\n",
        "\n",
        "def ethnicityToStr(num):\n",
        "    return ETHNICITY[num]\n",
        "\n",
        "def arrayEthnicityToStr(data):\n",
        "    res = []\n",
        "    for (_, __, ethnicity) in data:\n",
        "        res.append(ethnicityToStr(ethnicity))\n",
        "    return np.array(res)\n",
        "\n",
        "def histPlot(labels, title, yLabel, xLabel, bins):\n",
        "    plt.title(title, size=16)\n",
        "    sns.histplot(x = labels, bins = bins)\n",
        "    plt.ylabel(yLabel, size=12)\n",
        "    plt.xlabel(xLabel, size=12)\n",
        "    sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "    plt.show()\n",
        "\n",
        "def countPlot(labels, title, yLabel, xLabel):\n",
        "    plt.title(title, size=16)\n",
        "    ax = sns.countplot(x = labels)\n",
        "    plt.ylabel(yLabel, size=12)\n",
        "    plt.xlabel(xLabel, size=12)\n",
        "    sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "\n",
        "    total = len(labels)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        percentage = f'{100 * height / total:.1f}%'\n",
        "        ax.text(p.get_x() + p.get_width() / 2,\n",
        "                height + 5,\n",
        "                percentage,\n",
        "                ha='center')\n",
        "        \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FddWCNRzZjsU",
        "outputId": "5259e8a9-458c-41fa-a13f-8ba0594fd305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UTKFace already downloaded.\n"
          ]
        }
      ],
      "source": [
        "dataPath = 'UTKFace'\n",
        "if (dataPath not in os.listdir()):\n",
        "    print(\"Downloading UTKFace...\")\n",
        "    url = \"https://drive.google.com/uc?export=download&id=0BxYys69jI14kYVM3aVhKS1VhRUk&confirm=t&uuid=f981ca1d-ba0f-40c9-a4a0-8eaa887f3b6d&at=ANzk5s7e36SgjT0FlqBbRiijefRg:1681897584880\"\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    file = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
        "    file.extractall(path=\".\")\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(\"UTKFace already downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SKXElAb3ZjsU"
      },
      "outputs": [],
      "source": [
        "#data = []\n",
        "#labels = []\n",
        "\n",
        "#for imagePath in os.listdir(dataPath):\n",
        "#    try:\n",
        "#        imageTensor = torchvision.io.read_image(f'{dataPath}/{imagePath}').float().half()\n",
        "#        fileName = imagePath.split('_')\n",
        "#        labels.append((int(fileName[0]), int(fileName[1]), int(fileName[2])))\n",
        "#        data.append(imageTensor)\n",
        "#    except:\n",
        "#        pass\n",
        "#data = torch.stack(data).to(device)\n",
        "#labels = torch.Tensor(labels).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataPath, transform=None):\n",
        "        self.dataPath = dataPath\n",
        "        self.transform = transform\n",
        "        self.imagePaths = [f for f in os.listdir(self.dataPath) if f.endswith('.jpg')]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        imagePath = self.imagePaths[index]\n",
        "        try:\n",
        "            imageTensor = torchvision.io.read_image(f'{self.dataPath}/{imagePath}').float()\n",
        "            fileName = imagePath.split('_')\n",
        "            label = torch.Tensor([int(fileName[0]), int(fileName[1]), int(fileName[2])])\n",
        "            if self.transform:\n",
        "                imageTensor = self.transform(imageTensor)\n",
        "            return imageTensor, label\n",
        "        except:\n",
        "            return None\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.imagePaths)"
      ],
      "metadata": {
        "id": "p808DasNdMtZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dFTvbBTeZjsX",
        "outputId": "e0669894-83b4-4320-b1a7-c177273e9498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23708 2370 21338\n"
          ]
        }
      ],
      "source": [
        "# Load and normalizde the data\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize(224) #,\n",
        "     # transforms.ToTensor(),\n",
        "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "#dataset = torch.utils.data.TensorDataset(data, labels)\n",
        "\n",
        "dataset = MyDataset(dataPath, transform=transform)\n",
        "\n",
        "batchSize = 100\n",
        "testSplit = 0.1 # use 10% of dataset as test\n",
        "validSplit = 0.2 / (1-testSplit) # use 20% of dataset as validation\n",
        "\n",
        "testSize = int(np.floor(len(dataset)*testSplit))\n",
        "trainValidSize = int(np.ceil(len(dataset)*(1-testSplit)))\n",
        "validSize = int(np.ceil(trainValidSize*validSplit))\n",
        "trainSize = int(np.floor(trainValidSize*(1-validSplit)))\n",
        "print(len(dataset), testSize, trainValidSize)\n",
        "\n",
        "trainValidSet, testSet = torch.utils.data.random_split(dataset, [trainValidSize, testSize])\n",
        "trainSet, validSet = torch.utils.data.random_split(trainValidSet, [trainSize, validSize])\n",
        "\n",
        "trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize, shuffle=True)\n",
        "validLoader = torch.utils.data.DataLoader(validSet, batch_size=batchSize, shuffle=True)\n",
        "testLoader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rOCaZIQkZjsY"
      },
      "outputs": [],
      "source": [
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetModel,self).__init__()\n",
        "        self.resnet = torchvision.models.resnet18(weights=torchvision.models.resnet.ResNet18_Weights.DEFAULT)\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad=False\n",
        "        self.resnet.fc = nn.Linear(512, 512)\n",
        "        self.ageFc = nn.Linear(512,1)\n",
        "        self.genderFc = nn.Linear(512,2)\n",
        "        self.ethnicityFc = nn.Linear(512,5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        resOut = F.relu(self.resnet.forward(x))\n",
        "        ageOut = self.ageFc.forward(resOut)\n",
        "        genderOut = self.ageFc.forward(resOut)\n",
        "        ethnicityOut = self.ageFc.forward(resOut)\n",
        "        #genderOut = F.sigmoid(self.ageFc.forward(resOut))\n",
        "        #ethnicityOut = F.softmax(self.ageFc.forward(resOut))\n",
        "        return ageOut, genderOut, ethnicityOut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EyEWr79LZjsY"
      },
      "outputs": [],
      "source": [
        "# ResNetModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PPTkQJzyZjsY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lossAge(predictAge, targetAge):\n",
        "    loss = F.mse_loss(predictAge, targetAge)\n",
        "    return loss\n",
        "\n",
        "def lossGender(predictGender, targetGender):\n",
        "    loss = F.binary_cross_entropy(predictGender, targetGender)\n",
        "    return loss\n",
        "\n",
        "def lossEthnicity(predictEthnicity, targetEthnicity):\n",
        "    loss = F.cross_entropy(predictEthnicity, targetEthnicity)\n",
        "    return loss\n",
        "\n",
        "def lossFunction(predictAge, predictGender, predictEthnicity, targetAge, targetGender, targetEthnicity):\n",
        "    alpha = 1/3 # weight for age prediction\n",
        "    beta = 1/3 # weight for gender prediction\n",
        "    gamma = 1/3 # weight for ethncity prediction\n",
        "    ageLoss = lossAge(predictAge, targetAge)\n",
        "    genderLoss = lossGender(predictGender, targetGender)\n",
        "    ethnicityLoss = lossEthnicity(predictEthnicity, targetEthnicity)\n",
        "    totalLoss = alpha * ageLoss + beta * genderLoss + gamma * ethnicityLoss\n",
        "    return totalLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VyGFkzdvZjsZ"
      },
      "outputs": [],
      "source": [
        "def trainNetwork(model, optimizer, lossFunction, trainLoader, validLoader, epochs, device):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        ### TRAINING ###\n",
        "        trainLoss = 0\n",
        "        correctTrain = 0\n",
        "        totalTrain = 0\n",
        "        for batch_nr, (images, labels) in enumerate(trainLoader):\n",
        "            # Move data to GPU (if exists)\n",
        "            images, labels = images.to(device), labels.to(device)  \n",
        "\n",
        "            # Predict\n",
        "            agePredictions, genderPredictions, ethnicityPredictions = model(images)\n",
        "\n",
        "            # Get loss and backpropogate\n",
        "            loss = lossFunction(agePredictions, genderPredictions, ethnicityPredictions, \n",
        "                                labels[:, 0].view(-1, 1), labels[:, 1].view(-1, 1), labels[:, 2].view(-1, 1))\n",
        "            print(loss)\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimize parameters (weights and biases) and remove gradients after\n",
        "            optimizer.step() \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Save loss for whole epoch\n",
        "            trainLoss += loss.item()\n",
        "            \n",
        "            # Calculate training accuracy\n",
        "            # _, predictions = torch.max(predictions, 1) \n",
        "            # correctTrain += (predictions == labels).sum().item() \n",
        "            # totalTrain += len(images)\n",
        "\n",
        "            #print(f'Epoch [{epoch+1}/{epochs}] Batch [{batch_nr}/{len(trainLoader)}]')\n",
        "        \n",
        "\n",
        "        trainLoss /= len(trainLoader)\n",
        "        trainAccuracy = 100 * correctTrain / totalTrain\n",
        "\n",
        "        ### VALIDATION ###\n",
        "        validLoss = 0\n",
        "        correctValid = 0\n",
        "        totalValid = 0\n",
        "        for batch_nr, (images, labels) in enumerate(validLoader):\n",
        "            # Move data to GPU (if exists)\n",
        "            images, labels = images.to(device), labels.to(device) \n",
        "\n",
        "            # Predict            \n",
        "            agePredictions, genderPredictions, ethnicityPredictions = model(images)\n",
        "\n",
        "            # Get loss\n",
        "            loss = lossFunction(agePredictions, genderPredictions, ethnicityPredictions, \n",
        "                                labels[:, 0].view(-1, 1), labels[:, 1].view(-1, 1), labels[:, 2].view(-1, 1))\n",
        "\n",
        "            # Save loss for whole epoch\n",
        "            validLoss += loss.item()\n",
        "\n",
        "            # Calculate vaildation accuracy\n",
        "            #_, predictions = torch.max(predictions, 1) \n",
        "            #correctValid += (predictions == labels).sum().item() \n",
        "            #totalValid += len(images)\n",
        "\n",
        "            #print(f'Epoch [{epoch+1}/{epochs}] Batch [{batch_nr}/{len(validLoader)}]')\n",
        "\n",
        "        validLoss /= len(validLoader)\n",
        "        validAccuracy = 100 * correctValid / totalValid\n",
        "\n",
        "        # Print reuslt of epoch\n",
        "        print(f'Epoch [{epoch+1}/{epochs}] \\t Training Loss: {round(trainLoss, 4)} \\t Validation Loss: {round(validLoss, 4)} \\t Traning Acc: {round(trainAccuracy, 2)}% \\t Validation Acc: {round(validAccuracy, 2)}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "learningRate = 1e-3\n",
        "resnetModel = ResNetModel().to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(resnetModel.parameters(), lr=learningRate)\n",
        "trainNetwork(resnetModel, optimizer, lossFunction, trainLoader, validLoader, epochs, device)"
      ],
      "metadata": {
        "id": "shpRsm7hi3bg",
        "outputId": "b4ccab68-1dc0-4c75-d438-6eafb095c35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1703dd002596>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnetModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnetModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-3111661dc47e>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(model, optimizer, lossFunction, trainLoader, validLoader, epochs, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m             loss = lossFunction(agePredictions, genderPredictions, ethnicityPredictions, \n\u001b[1;32m     18\u001b[0m                                 labels[:, 0].view(-1, 1), labels[:, 1].view(-1, 1), labels[:, 2].view(-1, 1))\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    424\u001b[0m             )\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     def backward(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0mguard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DisableFuncTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    565\u001b[0m                         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    325\u001b[0m         )\n\u001b[1;32m    326\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             nonzero_finite_vals = torch.masked_select(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mtensor_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "QtfZSqAgzatB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0e46589cfc653cddf5fb35f750697a95da7a749fba015bbad1355989ab2e8d35"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}